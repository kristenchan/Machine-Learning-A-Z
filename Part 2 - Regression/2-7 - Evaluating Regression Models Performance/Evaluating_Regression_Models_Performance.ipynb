{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Evaluating Regression Models Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  R-Squared Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "課程範例程式及資料檔下載網址： https://www.superdatascience.com/machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $R^2$ 判別係數（Coefficient of determination）\n",
    "\n",
    "在 Liner Regression 中確定變數具有顯著線性關係後，會利用判定係數（Coefficient of determination）來判斷建立出來的 Model 中有多少變異是可以被解釋變數給解釋或是說反應變數 $Y$ 被解釋變數 $X$ 所解釋的比率，稱為 Liner Regression 的配適度 (Goodness of fit) 或稱為 Liner Regression 的解釋能力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div class=\"alert alert-block alert-info\"> Review </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](plot_2_7_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 Linear Regression 中，希望實際值 $y$ 跟預測值 $\\hat{y}$ 間越小越好，即 $SSE = \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$ 越小越好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事實上，在 Linear Regression 中，還有其他衡量變異的指標，來評估變異的情形"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "SST \\sum_{i=1}^{n}(y_i - \\bar{y})^2 = SSR \\sum_{i=1}^{n}(\\hat{y_i} - \\bar{y})^2 + SSE \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "> [Note] Linear Regression 中，有幾個衡量變異的指標：\n",
    "> 1. 總變異量 Total variation / Total Sum of Squares： $ SST = SS_{tot} = \\sum_{i=1}^{n}(y_i - \\bar{y})^2$\n",
    "> 2. 迴歸可解釋的變異量 Explained Variation / Regression Sum of Squares ： $ SSR = SS_{reg} = \\sum_{i=1}^{n}(\\hat{y_i} - \\bar{y})^2$ \n",
    "> 3. 誤差的變異量 Unexplained variation / Error Sum of Squares ： $ SSE = SS_{res} = \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$ \n",
    "> ![](plot_2_7_3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根據 $R^2$ 的定義， Model 中有多少變異是可以被解釋變數給解釋，所以 $R^2$ 可以表示為：\n",
    "<br><br>\n",
    "$$\n",
    "R^2 = 1 - \\frac{SSE}{SST} = \\frac{SSR}{SST}\n",
    "$$\n",
    "<br><br>\n",
    "$$\n",
    "0 \\leq R^2 \\leq 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](plot_2_7_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font class=\"alert alert-block alert-danger\"> !注意 </font> $R^2$ 越高，表示 Liner Regression 中可以解釋變異是越多。若 $R^2 \\to 1 $ 表示這個 Model 越有解釋能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div class=\"alert alert-block alert-warning\"> My Note </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](plot_2_7_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. 完全配適，此種情形下 $SSE=0$ ，因此 $R^2 = 1$，表示 Model 中的總變異是可以完全被解釋變數給解釋 <br>\n",
    "B. 配適不全，此種情形下 $SSR=0$ ，因此 $R^2 = 0$，表示 解釋變數無法解釋 Model 中的總變異"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusted R-Squared Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $Adjusted\\, R^2$ 調整後判別係數（Adjusted coefficient of determination）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ $R^2$ 判定係數（Coefficient of determination）容易受到樣本數及解釋變數 $X$ 個數的影響，當<span style=\"color:blue\">樣本數太小</span>或<span style=\"color:blue\">解釋變數個數太多</span>時， $R^2$ 就會被高估。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 將 $R^2$ 加入自由度做調整後，就是 $Adjusted\\, R^2$ ，其中 $p$ 為解釋變數 $X$ 的個數， $n$ 為樣本數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Adjusted\\, R^2 = 1 - (1 - R^2) \\times\\frac{n-1}{n-p-1} = 1 - \\frac{SSE}{SST} \\times\\frac{n-1}{n-p-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ $Adjusted\\, R^2$ 中 $\\frac{n-1}{n-p-1}$ 可以視為一個懲罰項。當解釋變數太多即 $p$ 增加，會使得整個懲罰項就會變大\n",
    "+ 當解釋變數增加，$R^2$ 會一直變大，但 $Adjusted\\, R^2$ 可以受到懲罰項的調節：\n",
    "    - 當新增的變數可以幫助 Model ，則不會受到懲罰項的影響\n",
    "    - 當新增的變數無法幫助 Model ，則會受到懲罰項的調節，進而變小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Regression Models Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "利用 $R^2$ 和 $Adjusted\\, R^2$ 的判斷可以來優化 Backward Elimintion 或其他選取法選出的 Model，這裡舉出四個例子，來表示當變數增加時$R^2$也會隨之增加，因此使用 $Adjusted\\, R^2$ 可能會較好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](plot_2_7_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A. 解釋變數：5 ； $R^2$：0.9505 ； $Adjusted\\, R^2$：0.9452 <br>\n",
    "B. 解釋變數：3 ； $R^2$：0.9507 ； $Adjusted\\, R^2$：0.9475 <br>\n",
    "C. 解釋變數：2 ； $R^2$：0.9505 ； $Adjusted\\, R^2$：0.9483 <br>\n",
    "D. 解釋變數：1 ； $R^2$：0.9465 ； $Adjusted\\, R^2$：0.9454 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [Note] <br>\n",
    "> 因為<span style=\"color:blue\">解釋變數個數變多</span>時， $R^2$ 容易被高估，因此利用 $Adjusted\\, R^2$ 來做判斷，在這裡會選擇 C Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Linear Regression Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 Linear Regression 中估計出來的係數該如何解釋呢？\n",
    "1. Estimate Coefficients >0 ，就表示該解釋變數 (Independent variables) 與反應變數 (Dependent variables) 間呈現正相關；反之，為負相關。\n",
    "2. Estimate Coefficients 的值則表示當該解釋變數 (Independent variables)增加一單位會影響反應變數 (Dependent variables) 的程度有多少。\n",
    "> [Note] <br>\n",
    "> 不同解釋變數間與反應變數的影響程度，不能以 Estimate Coefficient 的大小來衡量，因為解釋變數的單位可能不同；但是當解釋變數間的單位相同時，就可以這樣解釋。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](plot_2_7_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion of Part 2 - Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After learning about these six regression models, you are probably asking yourself the following questions：\n",
    "1. What are the pros and cons of each model ?\n",
    "2. How do I know which model to choose for my problem ?\n",
    "3. How can I improve each of these models ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's answer each of these questions one by one：\n",
    "1. What are the pros and cons of each model ? <br>\n",
    "Please find here a cheat-sheet that gives you all the pros and the cons of each regression model.\n",
    "<br><br>\n",
    "2. How do I know which model to choose for my problem ?<br>\n",
    "First, you need to figure out whether your problem is linear or non linear. You will learn how to do that in Part 10 - Model Selection. Then：<br>\n",
    "If your problem is linear, you should go for Simple Linear Regression if you only have one feature, and Multiple Linear Regression if you have several features.<br>\n",
    "If your problem is non linear, you should go for Polynomial Regression, SVR, Decision Tree or Random Forest. Then which one should you choose among these four ? That you will learn in Part 10 - Model Selection. The method consists of using a very relevant technique that evaluates your models performance, called k-Fold Cross Validation, and then picking the model that shows the best results. Feel free to jump directly to Part 10 if you already want to learn how to do that.\n",
    "<br><br>\n",
    "3. How can I improve each of these models ?<br>\n",
    "In Part 10 - Model Selection, you will find the second section dedicated to Parameter Tuning, that will allow you to improve the performance of your models, by tuning them. You probably already noticed that each model is composed of two types of parameters：<br>\n",
    "  + the parameters that are learnt, for example the coefficients in Linear Regression,\n",
    "  + the hyperparameters. <br> \n",
    "  \n",
    "  The hyperparameters are the parameters that are not learnt and that are fixed values inside the model equations. For example, the regularization parameter lambda or the penalty parameter C are hyperparameters. So far we used the default value of these hyperparameters, and we haven't searched for their optimal value so that your model reaches even higher performance. Finding their optimal value is exactly what Parameter Tuning is about. So for those of you already interested in improving your model performance and doing some parameter tuning, feel free to jump directly to Part 10 - Model Selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS -- Regularization Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting 問題：\n",
    "+ 可以看出資料點 (紅色) 完全配適這條曲線\n",
    "+ 有兩個新增的資料點 (綠色) 進來時，發現和 Model 間有很大的差距"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](plot_2_7_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解決 Overfitting 問題，可以參考正規化 (Regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](plot_2_7_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No Regularization\n",
    "$$\n",
    "Minimize \\sum_{i=1}^{n}(y^i - ( b_0 + b_1 x_1^i + ... +  b_m x_m^i ))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization -- Ridge Regression\n",
    "$$\n",
    "Minimize \\sum_{i=1}^{n}(y^i - ( b_0 + b_1 x_1^i + ... +  b_m x_m^i ))^2 + \\lambda( b_1^2 + ... +  b_m^2 )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization -- Lasso\n",
    "$$\n",
    "Minimize \\sum_{i=1}^{n}(y^i - ( b_0 + b_1 x_1^i + ... +  b_m x_m^i ))^2 + \\lambda( |b_1| + ... +  |b_m| )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization -- Elastic Net\n",
    "$$\n",
    "Minimize \\sum_{i=1}^{n}(y^i - ( b_0 + b_1 x_1^i + ... +  b_m x_m^i ))^2 + \\lambda_1( |b_1| + ... +  |b_m| ) + \\lambda_2( b_1^2 + ... +  b_m^2 )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](plot_2_7_9.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
